{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R5Curh6dSsJ"
      },
      "outputs": [],
      "source": [
        "#Evaluate the performance of a model:\n",
        "#(1)Boosting (2)Bagging\n",
        "#(3)Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Theory\n",
        "'''\n",
        "Eg:\n",
        "Boosting:\n",
        "Suppose you're a student preparing for a spelling bee competition. Boosting is like having a tutor who focuses on the words you keep misspelling.\n",
        "Each time you practice, your tutor helps you learn from your mistakes and gets you ready for the competition.\n",
        "\n",
        "Bagging:\n",
        "Now, imagine you're trying to make an important decision, like where to invest your money. Bagging is like asking many different financial advisors for their opinions. You combine all their advice to make the best investment choices and\n",
        "reduce the risk of making a bad decision.\n",
        "\n",
        "Random Forest:\n",
        "Suppose you're trying to predict which movie a person will enjoy based on their past movie preferences.\n",
        " Random Forest is like asking a group of movie critics, each with their own tastes and expertise, for their recommendations.\n",
        "  By combining all their opinions, you can make a more accurate prediction of which movie the person will like.\n",
        "\n",
        "\n",
        "Question: What is boosting in machine learning?\n",
        "Answer: Boosting is an ensemble learning technique that combines multiple weak learners sequentially to create a strong learner.\n",
        "It focuses on training each subsequent model to correct the errors of the previous ones.\n",
        "\n",
        "Question: How do you evaluate the performance of a boosting model?\n",
        "Answer: The performance of a boosting model can be evaluated using various metrics such as accuracy, precision, recall, F1-score, and ROC-AUC score.\n",
        " Additionally, you can examine learning curves to assess the model's convergence and potential overfitting.\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Question: What is bagging in machine learning?\n",
        "Answer: Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that combines the predictions of multiple independent models\n",
        "trained on different subsets of the training data. It aims to reduce variance and improve the stability of the model.\n",
        "\n",
        "Question: How do you evaluate the performance of a bagging model?\n",
        "Answer: Similar to boosting, the performance of a bagging model can be evaluated using metrics such as accuracy, precision,\n",
        "recall, F1-score, and ROC-AUC score. Additionally, you can assess the diversity among the base learners and examine their individual performances.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Question: What is Random Forest?\n",
        "Answer: Random Forest is an ensemble learning technique that builds multiple decision trees during training and combines their predictions\n",
        " through averaging or voting. It introduces randomness in the tree-building process to improve generalization and reduce overfitting.\n",
        "\n",
        "Question: How do you evaluate the performance of a Random Forest model?\n",
        "Answer: The performance of a Random Forest model can be evaluated using the same metrics as other classification models, such as accuracy,\n",
        "precision, recall, F1-score, and ROC-AUC score. Additionally, you can analyze feature importance to understand the contribution of each feature to the model's predictions. Furthermore, examining the out-of-bag error estimate can provide an unbiased estimate of the model's performance without the need for a separate validation set.\n",
        "'''"
      ],
      "metadata": {
        "id": "jdwAKT7IqQKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate some random classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define classifiers\n",
        "boosting_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "bagging_classifier = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "boosting_classifier.fit(X_train, y_train)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "boosting_pred = boosting_classifier.predict(X_test)\n",
        "bagging_pred = bagging_classifier.predict(X_test)\n",
        "random_forest_pred = random_forest_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "boosting_accuracy = accuracy_score(y_test, boosting_pred)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "random_forest_accuracy = accuracy_score(y_test, random_forest_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Boosting Accuracy:\", boosting_accuracy)\n",
        "print(\"Bagging Accuracy:\", bagging_accuracy)\n",
        "print(\"Random Forest Accuracy:\", random_forest_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq_qy8pQdzMZ",
        "outputId": "e37a3742-92f5-4cda-ca91-aac2b8e06fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting Accuracy: 0.87\n",
            "Bagging Accuracy: 0.885\n",
            "Random Forest Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define classifiers\n",
        "boosting_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "bagging_classifier = BaggingClassifier(n_estimators=50, random_state=42)\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train classifiers\n",
        "boosting_classifier.fit(X_train, y_train)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "boosting_pred = boosting_classifier.predict(X_test)\n",
        "bagging_pred = bagging_classifier.predict(X_test)\n",
        "random_forest_pred = random_forest_classifier.predict(X_test)\n",
        "\n",
        "# Evaluation metrics for Boosting\n",
        "accuracy = accuracy_score(y_test, boosting_pred)\n",
        "precision = precision_score(y_test, boosting_pred, average='macro')\n",
        "recall = recall_score(y_test, boosting_pred, average='macro')\n",
        "f1 = f1_score(y_test, boosting_pred, average='macro')\n",
        "print(\"Boosting Performance:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "\n",
        "# Evaluation metrics for Bagging\n",
        "accuracy = accuracy_score(y_test, bagging_pred)\n",
        "precision = precision_score(y_test, bagging_pred, average='macro')\n",
        "recall = recall_score(y_test, bagging_pred, average='macro')\n",
        "f1 = f1_score(y_test, bagging_pred, average='macro')\n",
        "print(\"Bagging Performance:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n",
        "\n",
        "# Evaluation metrics for Random Forest\n",
        "accuracy = accuracy_score(y_test, random_forest_pred)\n",
        "precision = precision_score(y_test, random_forest_pred, average='macro')\n",
        "recall = recall_score(y_test, random_forest_pred, average='macro')\n",
        "f1 = f1_score(y_test, random_forest_pred, average='macro')\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I16xPiGLd-QM",
        "outputId": "c9bb1f0f-5a4d-4a65-d4c0-591223004e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting Performance:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Bagging Performance:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Random Forest Performance:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22KMdq70fgg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}